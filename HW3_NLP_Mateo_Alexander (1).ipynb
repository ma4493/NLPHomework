{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73c247a4-343d-47de-8ab4-c9a77849fcc8",
   "metadata": {},
   "source": [
    "**Mateo Alexander**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5055160b-8e8a-43e2-835b-36846b59e30d",
   "metadata": {},
   "source": [
    "**Homework 3**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eaca9b5-92ca-4acf-916c-7f380af10d75",
   "metadata": {},
   "source": [
    "**Natural Language Processing: QMSS 5067**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35eec392-33bc-4f6b-b735-00246774110d",
   "metadata": {},
   "source": [
    "**Professor Patrick Houlihan**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133f4bb3-8965-49d7-adf8-07128aac70e8",
   "metadata": {},
   "source": [
    "**Homework Due 11/08/2024**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f218cf1-5d74-4396-8007-74e3637bc4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Build a function, called word_prob, that outputs probabilities, for every possible topic [all, fishing, hiking, machinelearning, mathematics] that a token or  sequential token combination (the user input to the function) shows up in an arbitrary textual based column  (body, body_sw, body_sw_stem), dictated by the user, from the dataframe, the_data, we have been using  in class.  The output dictionary of the function needs to have the following keys:\n",
    "\n",
    "all: <probability the sequential input token(s) shows up in ALL the corpuses\n",
    "fishing: <probability the sequential input token(s) shows up in the fishing corpuses\n",
    "hiking: <probability the sequential input token(s) shows up in the hiking corpuses\n",
    "machinelearning: <probability the sequential input token(s) shows up in the machinelearning corpuses\n",
    "mathematics: <probability the sequential input token(s) shows up in the mathematics corpuses\n",
    " \n",
    "The 'value' field of a dictionary is to have a value of None if the token(s) do not show up\n",
    "NOTE: If there are a total of 100 tokens, and the count of a specific token is 12, the probability of that token showing up is 12/100=.12\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ad6c7deb-73b8-45d5-a1a3-b87e96a2650d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in links: /usr/share/pip-wheels\n",
      "Requirement already satisfied: vaderSentiment in /home/37b4d573-02a2-4c75-b69b-8f21f8c5d212/.local/lib/python3.10/site-packages (3.3.2)\n",
      "Requirement already satisfied: requests in /opt/conda/envs/anaconda-2024.02-py310/lib/python3.10/site-packages (from vaderSentiment) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/anaconda-2024.02-py310/lib/python3.10/site-packages (from requests->vaderSentiment) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/anaconda-2024.02-py310/lib/python3.10/site-packages (from requests->vaderSentiment) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/anaconda-2024.02-py310/lib/python3.10/site-packages (from requests->vaderSentiment) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/anaconda-2024.02-py310/lib/python3.10/site-packages (from requests->vaderSentiment) (2024.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "a5bd11e3-0ecd-4e8c-a0d8-61ca74b88962",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import urllib.request\n",
    "from collections import Counter\n",
    "import itertools\n",
    "from itertools import islice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "80bfa603-40d6-4934-87c5-83a44aad59ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import read_file, clean_txt\n",
    "from utils import stem_fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "75dfb816-8fa4-4337-99d2-369d525e4e8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/37b4d573-02a2-4c75-b69b-8f21f8c5d212/Natural Language Processing/Natural Language Processing/Data\n"
     ]
    }
   ],
   "source": [
    "file_path_data = os.path.abspath('Natural Language Processing/Data')\n",
    "print(file_path_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "c6b9cca7-c055-4330-9cbf-0b3f9954d7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish the directory\n",
    "directory = '/home/37b4d573-02a2-4c75-b69b-8f21f8c5d212/Natural Language Processing/Data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "633aad8c-8465-47e0-9b72-611fa215f5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version 6: Implement Clean_Txt, Multiprocessing, Stemming, Batch Processing, and DataFrame Output\n",
    "def process_batch(batch):\n",
    "    all_tokens = Counter()\n",
    "    all_tokens_stemmed = Counter()\n",
    "    topic_tokens = {\"Fishing\": Counter(), \"Hiking\": Counter(), \"MachineLearning\": Counter(), \"Mathematics\": Counter()}\n",
    "    topic_tokens_stemmed = {\"Fishing\": Counter(), \"Hiking\": Counter(), \"MachineLearning\": Counter(), \"Mathematics\": Counter()}\n",
    "    \n",
    "    for topic, file_path in batch:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            body_text = f.read()\n",
    "            # Clean the text\n",
    "            body_text = clean_txt(body_text)\n",
    "            tokens = body_text.split()\n",
    "            all_tokens.update(tokens)\n",
    "            topic_tokens[topic].update(tokens)\n",
    "            # Apply stemming to tokens\n",
    "            stemmed_tokens = [stem_fun(token, \"stem\") for token in tokens]\n",
    "            all_tokens_stemmed.update(stemmed_tokens)\n",
    "            topic_tokens_stemmed[topic].update(stemmed_tokens)\n",
    "    \n",
    "    return all_tokens, all_tokens_stemmed, topic_tokens, topic_tokens_stemmed\n",
    "\n",
    "def batch_iterator(iterable, batch_size):\n",
    "    iterator = iter(iterable)\n",
    "    for first in iterator:\n",
    "        yield list(islice(iterator, batch_size - 1)) + [first]\n",
    "\n",
    "def word_prob(directory, column_name, token, batch_size=10):\n",
    "    # Normalize the token\n",
    "    token_cleaned = clean_txt(token)\n",
    "    token_stemmed = stem_fun(token_cleaned, \"stem\")\n",
    "    \n",
    "    # Initialize counters\n",
    "    all_tokens = Counter()\n",
    "    all_tokens_stemmed = Counter()\n",
    "    topic_tokens = {\n",
    "        \"Fishing\": Counter(),\n",
    "        \"Hiking\": Counter(),\n",
    "        \"MachineLearning\": Counter(),\n",
    "        \"Mathematics\": Counter()\n",
    "    }\n",
    "    topic_tokens_stemmed = {\n",
    "        \"Fishing\": Counter(),\n",
    "        \"Hiking\": Counter(),\n",
    "        \"MachineLearning\": Counter(),\n",
    "        \"Mathematics\": Counter()\n",
    "    }\n",
    "    \n",
    "    # Collect files for all topics\n",
    "    files_to_process = []\n",
    "    for topic in topic_tokens.keys():\n",
    "        topic_path = os.path.join(directory, topic)\n",
    "        if not os.path.exists(topic_path):\n",
    "            raise ValueError(f\"Directory '{topic_path}' does not exist.\")\n",
    "        \n",
    "        for root, _, files in os.walk(topic_path):\n",
    "            for file in files:\n",
    "                if file.endswith(\".txt\"):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    files_to_process.append((topic, file_path))\n",
    "    \n",
    "    # Create batches of files\n",
    "    batches = list(batch_iterator(files_to_process, batch_size))\n",
    "    \n",
    "    # Use multiprocessing to process batches in parallel\n",
    "    with Pool(processes=multiprocessing.cpu_count()) as pool:\n",
    "        results = pool.map(process_batch, batches)\n",
    "    \n",
    "    # Update counters with results from parallel processing\n",
    "    for result in results:\n",
    "        batch_all_tokens, batch_all_tokens_stemmed, batch_topic_tokens, batch_topic_tokens_stemmed = result\n",
    "        all_tokens.update(batch_all_tokens)\n",
    "        all_tokens_stemmed.update(batch_all_tokens_stemmed)\n",
    "        for topic in topic_tokens.keys():\n",
    "            topic_tokens[topic].update(batch_topic_tokens[topic])\n",
    "            topic_tokens_stemmed[topic].update(batch_topic_tokens_stemmed[topic])\n",
    "    \n",
    "    # Calculate probabilities\n",
    "    def calc_probability(count, total):\n",
    "        return count / total if total > 0 else None\n",
    "\n",
    "    # Probabilities without stemming\n",
    "    all_count = sum(all_tokens.values())\n",
    "    token_count_all = all_tokens[token_cleaned]\n",
    "    probability_all = calc_probability(token_count_all, all_count)\n",
    "    \n",
    "    probabilities = {\n",
    "        \"all\": probability_all,\n",
    "        \"Fishing\": None,\n",
    "        \"Hiking\": None,\n",
    "        \"MachineLearning\": None,\n",
    "        \"Mathematics\": None\n",
    "    }\n",
    "    \n",
    "    for topic, counter in topic_tokens.items():\n",
    "        total_tokens_topic = sum(counter.values())\n",
    "        token_count_topic = counter[token_cleaned]\n",
    "        probabilities[topic] = calc_probability(token_count_topic, total_tokens_topic)\n",
    "    \n",
    "    # Probabilities with stemming\n",
    "    all_count_stemmed = sum(all_tokens_stemmed.values())\n",
    "    token_count_all_stemmed = all_tokens_stemmed[token_stemmed]\n",
    "    probability_all_stemmed = calc_probability(token_count_all_stemmed, all_count_stemmed)\n",
    "    \n",
    "    probabilities_stemmed = {\n",
    "        \"all\": probability_all_stemmed,\n",
    "        \"Fishing\": None,\n",
    "        \"Hiking\": None,\n",
    "        \"MachineLearning\": None,\n",
    "        \"Mathematics\": None\n",
    "    }\n",
    "    \n",
    "    for topic, counter in topic_tokens_stemmed.items():\n",
    "        total_tokens_topic_stemmed = sum(counter.values())\n",
    "        token_count_topic_stemmed = counter[token_stemmed]\n",
    "        probabilities_stemmed[topic] = calc_probability(token_count_topic_stemmed, total_tokens_topic_stemmed)\n",
    "    \n",
    "    # Replace probabilities with None if the token count is 0\n",
    "    for key, value in probabilities.items():\n",
    "        if value == 0:\n",
    "            probabilities[key] = None\n",
    "    for key, value in probabilities_stemmed.items():\n",
    "        if value == 0:\n",
    "            probabilities_stemmed[key] = None\n",
    "    \n",
    "    # Create a DataFrame for better display\n",
    "    data = []\n",
    "    data.append([\"All\", probabilities[\"all\"], probabilities_stemmed[\"all\"]])\n",
    "    for topic in topic_tokens.keys():\n",
    "        data.append([topic, probabilities[topic], probabilities_stemmed[topic]])\n",
    "    \n",
    "    df = pd.DataFrame(data, columns=[\"Topic\", \"Probability Without Stemming\", \"Probability With Stemming\"])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "9c2563c1-0e45-45bd-8356-81dcc6e60103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Topic  Probability Without Stemming  Probability With Stemming\n",
      "0              All                      0.002114                   0.004733\n",
      "1          Fishing                      0.000415                   0.000586\n",
      "2           Hiking                      0.006753                   0.015361\n",
      "3  MachineLearning                           NaN                        NaN\n",
      "4      Mathematics                           NaN                        NaN\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "result = word_prob(directory, \"body\", \"hiking\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "b9c7039a-3a62-42af-b402-c158217e4cc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef word_prob(directory, column_name, token):\\n    # Initialize counters\\n    all_tokens = Counter()\\n    topic_tokens = {\\n        \"Fishing\": Counter(),\\n        \"Hiking\": Counter(),\\n        \"MachineLearning\": Counter(),\\n        \"Mathematics\": Counter()\\n    }\\n    \\n    # Walk through the directory and collect token counts\\n    for topic in topic_tokens.keys():\\n        topic_path = os.path.join(directory, topic)\\n        if not os.path.exists(topic_path):\\n            raise ValueError(f\"Directory \\'{topic_path}\\' does not exist.\")\\n        \\n        for root, _, files in os.walk(topic_path):\\n            for file in files:\\n                if file.endswith(\".txt\"):\\n                    file_path = os.path.join(root, file)\\n                    with open(file_path, \\'r\\', encoding=\\'utf-8\\') as f:\\n                        body_text = f.read()\\n                        tokens = body_text.split()\\n                        # Update the counters for \"all\" tokens and topic tokens\\n                        all_tokens.update(tokens)\\n                        topic_tokens[topic].update(tokens)\\n    \\n    # Calculate probabilities\\n    def calc_probability(count, total):\\n        return count / total if total > 0 else None\\n\\n    all_count = sum(all_tokens.values())\\n    token_count_all = all_tokens[token]\\n    probability_all = calc_probability(token_count_all, all_count)\\n    \\n    # Calculate topic-specific probabilities\\n    probabilities = {\\n        \"All\": probability_all,\\n        \"Fishing\": None,\\n        \"Hiking\": None,\\n        \"MachineLearning\": None,\\n        \"Mathematics\": None\\n    }\\n    \\n    for topic, counter in topic_tokens.items():\\n        total_tokens_topic = sum(counter.values())\\n        token_count_topic = counter[token]\\n        probabilities[topic] = calc_probability(token_count_topic, total_tokens_topic)\\n    \\n    # Replace probabilities with None if the token count is 0\\n    for key, value in probabilities.items():\\n        if value == 0:\\n            probabilities[key] = None\\n    \\n    return probabilities\\n'"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Version 1: Basic Algorithm\n",
    "\"\"\"\n",
    "def word_prob(directory, column_name, token):\n",
    "    # Initialize counters\n",
    "    all_tokens = Counter()\n",
    "    topic_tokens = {\n",
    "        \"Fishing\": Counter(),\n",
    "        \"Hiking\": Counter(),\n",
    "        \"MachineLearning\": Counter(),\n",
    "        \"Mathematics\": Counter()\n",
    "    }\n",
    "    \n",
    "    # Walk through the directory and collect token counts\n",
    "    for topic in topic_tokens.keys():\n",
    "        topic_path = os.path.join(directory, topic)\n",
    "        if not os.path.exists(topic_path):\n",
    "            raise ValueError(f\"Directory '{topic_path}' does not exist.\")\n",
    "        \n",
    "        for root, _, files in os.walk(topic_path):\n",
    "            for file in files:\n",
    "                if file.endswith(\".txt\"):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                        body_text = f.read()\n",
    "                        tokens = body_text.split()\n",
    "                        # Update the counters for \"all\" tokens and topic tokens\n",
    "                        all_tokens.update(tokens)\n",
    "                        topic_tokens[topic].update(tokens)\n",
    "    \n",
    "    # Calculate probabilities\n",
    "    def calc_probability(count, total):\n",
    "        return count / total if total > 0 else None\n",
    "\n",
    "    all_count = sum(all_tokens.values())\n",
    "    token_count_all = all_tokens[token]\n",
    "    probability_all = calc_probability(token_count_all, all_count)\n",
    "    \n",
    "    # Calculate topic-specific probabilities\n",
    "    probabilities = {\n",
    "        \"All\": probability_all,\n",
    "        \"Fishing\": None,\n",
    "        \"Hiking\": None,\n",
    "        \"MachineLearning\": None,\n",
    "        \"Mathematics\": None\n",
    "    }\n",
    "    \n",
    "    for topic, counter in topic_tokens.items():\n",
    "        total_tokens_topic = sum(counter.values())\n",
    "        token_count_topic = counter[token]\n",
    "        probabilities[topic] = calc_probability(token_count_topic, total_tokens_topic)\n",
    "    \n",
    "    # Replace probabilities with None if the token count is 0\n",
    "    for key, value in probabilities.items():\n",
    "        if value == 0:\n",
    "            probabilities[key] = None\n",
    "    \n",
    "    return probabilities\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "4ea22ca6-adb9-44bc-89f2-f6ca909136bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef process_file(args):\\n    topic, file_path = args\\n    with open(file_path, \\'r\\', encoding=\\'utf-8\\') as f:\\n        body_text = f.read()\\n        tokens = body_text.split()\\n        return topic, tokens\\n\\ndef word_prob(directory, column_name, token):\\n    # Initialize counters\\n    all_tokens = Counter()\\n    topic_tokens = {\\n        \"Fishing\": Counter(),\\n        \"Hiking\": Counter(),\\n        \"MachineLearning\": Counter(),\\n        \"Mathematics\": Counter()\\n    }\\n    \\n    # Collect files for all topics\\n    files_to_process = []\\n    for topic in topic_tokens.keys():\\n        topic_path = os.path.join(directory, topic)\\n        if not os.path.exists(topic_path):\\n            raise ValueError(f\"Directory \\'{topic_path}\\' does not exist.\")\\n        \\n        for root, _, files in os.walk(topic_path):\\n            for file in files:\\n                if file.endswith(\".txt\"):\\n                    file_path = os.path.join(root, file)\\n                    files_to_process.append((topic, file_path))\\n    \\n    # Use multiprocessing to process files in parallel\\n    with Pool(processes=multiprocessing.cpu_count()) as pool:\\n        results = pool.map(process_file, files_to_process)\\n    \\n    # Update counters with results from parallel processing\\n    for topic, tokens in results:\\n        all_tokens.update(tokens)\\n        topic_tokens[topic].update(tokens)\\n    \\n    # Calculate probabilities\\n    def calc_probability(count, total):\\n        return count / total if total > 0 else None\\n\\n    all_count = sum(all_tokens.values())\\n    token_count_all = all_tokens[token]\\n    probability_all = calc_probability(token_count_all, all_count)\\n    \\n    # Calculate topic-specific probabilities\\n    probabilities = {\\n        \"all\": probability_all,\\n        \"Fishing\": None,\\n        \"Hiking\": None,\\n        \"MachineLearning\": None,\\n        \"Mathematics\": None\\n    }\\n    \\n    for topic, counter in topic_tokens.items():\\n        total_tokens_topic = sum(counter.values())\\n        token_count_topic = counter[token]\\n        probabilities[topic] = calc_probability(token_count_topic, total_tokens_topic)\\n    \\n    # Replace probabilities with None if the token count is 0\\n    for key, value in probabilities.items():\\n        if value == 0:\\n            probabilities[key] = None\\n    \\n    return probabilities\\n'"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Version 2: Multiprocessing Implemented\n",
    "\"\"\"\n",
    "def process_file(args):\n",
    "    topic, file_path = args\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        body_text = f.read()\n",
    "        tokens = body_text.split()\n",
    "        return topic, tokens\n",
    "\n",
    "def word_prob(directory, column_name, token):\n",
    "    # Initialize counters\n",
    "    all_tokens = Counter()\n",
    "    topic_tokens = {\n",
    "        \"Fishing\": Counter(),\n",
    "        \"Hiking\": Counter(),\n",
    "        \"MachineLearning\": Counter(),\n",
    "        \"Mathematics\": Counter()\n",
    "    }\n",
    "    \n",
    "    # Collect files for all topics\n",
    "    files_to_process = []\n",
    "    for topic in topic_tokens.keys():\n",
    "        topic_path = os.path.join(directory, topic)\n",
    "        if not os.path.exists(topic_path):\n",
    "            raise ValueError(f\"Directory '{topic_path}' does not exist.\")\n",
    "        \n",
    "        for root, _, files in os.walk(topic_path):\n",
    "            for file in files:\n",
    "                if file.endswith(\".txt\"):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    files_to_process.append((topic, file_path))\n",
    "    \n",
    "    # Use multiprocessing to process files in parallel\n",
    "    with Pool(processes=multiprocessing.cpu_count()) as pool:\n",
    "        results = pool.map(process_file, files_to_process)\n",
    "    \n",
    "    # Update counters with results from parallel processing\n",
    "    for topic, tokens in results:\n",
    "        all_tokens.update(tokens)\n",
    "        topic_tokens[topic].update(tokens)\n",
    "    \n",
    "    # Calculate probabilities\n",
    "    def calc_probability(count, total):\n",
    "        return count / total if total > 0 else None\n",
    "\n",
    "    all_count = sum(all_tokens.values())\n",
    "    token_count_all = all_tokens[token]\n",
    "    probability_all = calc_probability(token_count_all, all_count)\n",
    "    \n",
    "    # Calculate topic-specific probabilities\n",
    "    probabilities = {\n",
    "        \"all\": probability_all,\n",
    "        \"Fishing\": None,\n",
    "        \"Hiking\": None,\n",
    "        \"MachineLearning\": None,\n",
    "        \"Mathematics\": None\n",
    "    }\n",
    "    \n",
    "    for topic, counter in topic_tokens.items():\n",
    "        total_tokens_topic = sum(counter.values())\n",
    "        token_count_topic = counter[token]\n",
    "        probabilities[topic] = calc_probability(token_count_topic, total_tokens_topic)\n",
    "    \n",
    "    # Replace probabilities with None if the token count is 0\n",
    "    for key, value in probabilities.items():\n",
    "        if value == 0:\n",
    "            probabilities[key] = None\n",
    "    \n",
    "    return probabilities\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "3b3885a4-4326-47f3-9117-5c5fb0088033",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef process_file(args):\\n    topic, file_path = args\\n    with open(file_path, \\'r\\', encoding=\\'utf-8\\') as f:\\n        body_text = f.read()\\n        # Clean the text\\n        body_text = clean_txt(body_text)\\n        tokens = body_text.split()\\n        return topic, tokens\\n\\ndef word_prob(directory, column_name, token):\\n    # Normalize the token\\n    token = clean_txt(token)\\n    \\n    # Initialize counters\\n    all_tokens = Counter()\\n    topic_tokens = {\\n        \"Fishing\": Counter(),\\n        \"Hiking\": Counter(),\\n        \"MachineLearning\": Counter(),\\n        \"Mathematics\": Counter()\\n    }\\n    \\n    # Collect files for all topics\\n    files_to_process = []\\n    for topic in topic_tokens.keys():\\n        topic_path = os.path.join(directory, topic)\\n        if not os.path.exists(topic_path):\\n            raise ValueError(f\"Directory \\'{topic_path}\\' does not exist.\")\\n        \\n        for root, _, files in os.walk(topic_path):\\n            for file in files:\\n                if file.endswith(\".txt\"):\\n                    file_path = os.path.join(root, file)\\n                    files_to_process.append((topic, file_path))\\n    \\n    # Use multiprocessing to process files in parallel\\n    with Pool(processes=multiprocessing.cpu_count()) as pool:\\n        results = pool.map(process_file, files_to_process)\\n    \\n    # Update counters with results from parallel processing\\n    for topic, tokens in results:\\n        all_tokens.update(tokens)\\n        topic_tokens[topic].update(tokens)\\n    \\n    # Calculate probabilities\\n    def calc_probability(count, total):\\n        return count / total if total > 0 else None\\n\\n    all_count = sum(all_tokens.values())\\n    token_count_all = all_tokens[token]\\n    probability_all = calc_probability(token_count_all, all_count)\\n    \\n    # Calculate topic-specific probabilities\\n    probabilities = {\\n        \"all\": probability_all,\\n        \"Fishing\": None,\\n        \"Hiking\": None,\\n        \"MachineLearning\": None,\\n        \"Mathematics\": None\\n    }\\n    \\n    for topic, counter in topic_tokens.items():\\n        total_tokens_topic = sum(counter.values())\\n        token_count_topic = counter[token]\\n        probabilities[topic] = calc_probability(token_count_topic, total_tokens_topic)\\n    \\n    # Replace probabilities with None if the token count is 0\\n    for key, value in probabilities.items():\\n        if value == 0:\\n            probabilities[key] = None\\n    \\n    return probabilities\\n'"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Version 3: Clean Text Implemented\n",
    "\"\"\"\n",
    "def process_file(args):\n",
    "    topic, file_path = args\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        body_text = f.read()\n",
    "        # Clean the text\n",
    "        body_text = clean_txt(body_text)\n",
    "        tokens = body_text.split()\n",
    "        return topic, tokens\n",
    "\n",
    "def word_prob(directory, column_name, token):\n",
    "    # Normalize the token\n",
    "    token = clean_txt(token)\n",
    "    \n",
    "    # Initialize counters\n",
    "    all_tokens = Counter()\n",
    "    topic_tokens = {\n",
    "        \"Fishing\": Counter(),\n",
    "        \"Hiking\": Counter(),\n",
    "        \"MachineLearning\": Counter(),\n",
    "        \"Mathematics\": Counter()\n",
    "    }\n",
    "    \n",
    "    # Collect files for all topics\n",
    "    files_to_process = []\n",
    "    for topic in topic_tokens.keys():\n",
    "        topic_path = os.path.join(directory, topic)\n",
    "        if not os.path.exists(topic_path):\n",
    "            raise ValueError(f\"Directory '{topic_path}' does not exist.\")\n",
    "        \n",
    "        for root, _, files in os.walk(topic_path):\n",
    "            for file in files:\n",
    "                if file.endswith(\".txt\"):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    files_to_process.append((topic, file_path))\n",
    "    \n",
    "    # Use multiprocessing to process files in parallel\n",
    "    with Pool(processes=multiprocessing.cpu_count()) as pool:\n",
    "        results = pool.map(process_file, files_to_process)\n",
    "    \n",
    "    # Update counters with results from parallel processing\n",
    "    for topic, tokens in results:\n",
    "        all_tokens.update(tokens)\n",
    "        topic_tokens[topic].update(tokens)\n",
    "    \n",
    "    # Calculate probabilities\n",
    "    def calc_probability(count, total):\n",
    "        return count / total if total > 0 else None\n",
    "\n",
    "    all_count = sum(all_tokens.values())\n",
    "    token_count_all = all_tokens[token]\n",
    "    probability_all = calc_probability(token_count_all, all_count)\n",
    "    \n",
    "    # Calculate topic-specific probabilities\n",
    "    probabilities = {\n",
    "        \"all\": probability_all,\n",
    "        \"Fishing\": None,\n",
    "        \"Hiking\": None,\n",
    "        \"MachineLearning\": None,\n",
    "        \"Mathematics\": None\n",
    "    }\n",
    "    \n",
    "    for topic, counter in topic_tokens.items():\n",
    "        total_tokens_topic = sum(counter.values())\n",
    "        token_count_topic = counter[token]\n",
    "        probabilities[topic] = calc_probability(token_count_topic, total_tokens_topic)\n",
    "    \n",
    "    # Replace probabilities with None if the token count is 0\n",
    "    for key, value in probabilities.items():\n",
    "        if value == 0:\n",
    "            probabilities[key] = None\n",
    "    \n",
    "    return probabilities\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "2c264ad4-a621-455e-a6df-d751bdb754d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef process_file(args):\\n    topic, file_path = args\\n    with open(file_path, \\'r\\', encoding=\\'utf-8\\') as f:\\n        body_text = f.read()\\n        # Clean the text\\n        body_text = clean_txt(body_text)\\n        tokens = body_text.split()\\n        return topic, tokens\\n\\ndef word_prob(directory, column_name, token):\\n    # Normalize the token\\n    token_cleaned = clean_txt(token)\\n    token_stemmed = stem_fun(token_cleaned, \"stem\")\\n    \\n    # Initialize counters\\n    all_tokens = Counter()\\n    all_tokens_stemmed = Counter()\\n    topic_tokens = {\\n        \"Fishing\": Counter(),\\n        \"Hiking\": Counter(),\\n        \"MachineLearning\": Counter(),\\n        \"Mathematics\": Counter()\\n    }\\n    topic_tokens_stemmed = {\\n        \"Fishing\": Counter(),\\n        \"Hiking\": Counter(),\\n        \"MachineLearning\": Counter(),\\n        \"Mathematics\": Counter()\\n    }\\n    \\n    # Collect files for all topics\\n    files_to_process = []\\n    for topic in topic_tokens.keys():\\n        topic_path = os.path.join(directory, topic)\\n        if not os.path.exists(topic_path):\\n            raise ValueError(f\"Directory \\'{topic_path}\\' does not exist.\")\\n        \\n        for root, _, files in os.walk(topic_path):\\n            for file in files:\\n                if file.endswith(\".txt\"):\\n                    file_path = os.path.join(root, file)\\n                    files_to_process.append((topic, file_path))\\n    \\n    # Use multiprocessing to process files in parallel\\n    with Pool(processes=multiprocessing.cpu_count()) as pool:\\n        results = pool.map(process_file, files_to_process)\\n    \\n    # Update counters with results from parallel processing\\n    for topic, tokens in results:\\n        all_tokens.update(tokens)\\n        topic_tokens[topic].update(tokens)\\n        # Apply stemming to tokens\\n        stemmed_tokens = [stem_fun(token, \"stem\") for token in tokens]\\n        all_tokens_stemmed.update(stemmed_tokens)\\n        topic_tokens_stemmed[topic].update(stemmed_tokens)\\n    \\n    # Calculate probabilities\\n    def calc_probability(count, total):\\n        return count / total if total > 0 else None\\n\\n    # Probabilities without stemming\\n    all_count = sum(all_tokens.values())\\n    token_count_all = all_tokens[token_cleaned]\\n    probability_all = calc_probability(token_count_all, all_count)\\n    \\n    probabilities = {\\n        \"all\": probability_all,\\n        \"Fishing\": None,\\n        \"Hiking\": None,\\n        \"MachineLearning\": None,\\n        \"Mathematics\": None\\n    }\\n    \\n    for topic, counter in topic_tokens.items():\\n        total_tokens_topic = sum(counter.values())\\n        token_count_topic = counter[token_cleaned]\\n        probabilities[topic] = calc_probability(token_count_topic, total_tokens_topic)\\n    \\n    # Probabilities with stemming\\n    all_count_stemmed = sum(all_tokens_stemmed.values())\\n    token_count_all_stemmed = all_tokens_stemmed[token_stemmed]\\n    probability_all_stemmed = calc_probability(token_count_all_stemmed, all_count_stemmed)\\n    \\n    probabilities_stemmed = {\\n        \"all\": probability_all_stemmed,\\n        \"Fishing\": None,\\n        \"Hiking\": None,\\n        \"MachineLearning\": None,\\n        \"Mathematics\": None\\n    }\\n    \\n    for topic, counter in topic_tokens_stemmed.items():\\n        total_tokens_topic_stemmed = sum(counter.values())\\n        token_count_topic_stemmed = counter[token_stemmed]\\n        probabilities_stemmed[topic] = calc_probability(token_count_topic_stemmed, total_tokens_topic_stemmed)\\n    \\n    # Replace probabilities with None if the token count is 0\\n    for key, value in probabilities.items():\\n        if value == 0:\\n            probabilities[key] = None\\n    for key, value in probabilities_stemmed.items():\\n        if value == 0:\\n            probabilities_stemmed[key] = None\\n    \\n    return {\\n        \"without_stemming\": probabilities,\\n        \"with_stemming\": probabilities_stemmed\\n    }\\n'"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Version 4: Stemming Algorithm Implemented\n",
    "\"\"\"\n",
    "def process_file(args):\n",
    "    topic, file_path = args\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        body_text = f.read()\n",
    "        # Clean the text\n",
    "        body_text = clean_txt(body_text)\n",
    "        tokens = body_text.split()\n",
    "        return topic, tokens\n",
    "\n",
    "def word_prob(directory, column_name, token):\n",
    "    # Normalize the token\n",
    "    token_cleaned = clean_txt(token)\n",
    "    token_stemmed = stem_fun(token_cleaned, \"stem\")\n",
    "    \n",
    "    # Initialize counters\n",
    "    all_tokens = Counter()\n",
    "    all_tokens_stemmed = Counter()\n",
    "    topic_tokens = {\n",
    "        \"Fishing\": Counter(),\n",
    "        \"Hiking\": Counter(),\n",
    "        \"MachineLearning\": Counter(),\n",
    "        \"Mathematics\": Counter()\n",
    "    }\n",
    "    topic_tokens_stemmed = {\n",
    "        \"Fishing\": Counter(),\n",
    "        \"Hiking\": Counter(),\n",
    "        \"MachineLearning\": Counter(),\n",
    "        \"Mathematics\": Counter()\n",
    "    }\n",
    "    \n",
    "    # Collect files for all topics\n",
    "    files_to_process = []\n",
    "    for topic in topic_tokens.keys():\n",
    "        topic_path = os.path.join(directory, topic)\n",
    "        if not os.path.exists(topic_path):\n",
    "            raise ValueError(f\"Directory '{topic_path}' does not exist.\")\n",
    "        \n",
    "        for root, _, files in os.walk(topic_path):\n",
    "            for file in files:\n",
    "                if file.endswith(\".txt\"):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    files_to_process.append((topic, file_path))\n",
    "    \n",
    "    # Use multiprocessing to process files in parallel\n",
    "    with Pool(processes=multiprocessing.cpu_count()) as pool:\n",
    "        results = pool.map(process_file, files_to_process)\n",
    "    \n",
    "    # Update counters with results from parallel processing\n",
    "    for topic, tokens in results:\n",
    "        all_tokens.update(tokens)\n",
    "        topic_tokens[topic].update(tokens)\n",
    "        # Apply stemming to tokens\n",
    "        stemmed_tokens = [stem_fun(token, \"stem\") for token in tokens]\n",
    "        all_tokens_stemmed.update(stemmed_tokens)\n",
    "        topic_tokens_stemmed[topic].update(stemmed_tokens)\n",
    "    \n",
    "    # Calculate probabilities\n",
    "    def calc_probability(count, total):\n",
    "        return count / total if total > 0 else None\n",
    "\n",
    "    # Probabilities without stemming\n",
    "    all_count = sum(all_tokens.values())\n",
    "    token_count_all = all_tokens[token_cleaned]\n",
    "    probability_all = calc_probability(token_count_all, all_count)\n",
    "    \n",
    "    probabilities = {\n",
    "        \"all\": probability_all,\n",
    "        \"Fishing\": None,\n",
    "        \"Hiking\": None,\n",
    "        \"MachineLearning\": None,\n",
    "        \"Mathematics\": None\n",
    "    }\n",
    "    \n",
    "    for topic, counter in topic_tokens.items():\n",
    "        total_tokens_topic = sum(counter.values())\n",
    "        token_count_topic = counter[token_cleaned]\n",
    "        probabilities[topic] = calc_probability(token_count_topic, total_tokens_topic)\n",
    "    \n",
    "    # Probabilities with stemming\n",
    "    all_count_stemmed = sum(all_tokens_stemmed.values())\n",
    "    token_count_all_stemmed = all_tokens_stemmed[token_stemmed]\n",
    "    probability_all_stemmed = calc_probability(token_count_all_stemmed, all_count_stemmed)\n",
    "    \n",
    "    probabilities_stemmed = {\n",
    "        \"all\": probability_all_stemmed,\n",
    "        \"Fishing\": None,\n",
    "        \"Hiking\": None,\n",
    "        \"MachineLearning\": None,\n",
    "        \"Mathematics\": None\n",
    "    }\n",
    "    \n",
    "    for topic, counter in topic_tokens_stemmed.items():\n",
    "        total_tokens_topic_stemmed = sum(counter.values())\n",
    "        token_count_topic_stemmed = counter[token_stemmed]\n",
    "        probabilities_stemmed[topic] = calc_probability(token_count_topic_stemmed, total_tokens_topic_stemmed)\n",
    "    \n",
    "    # Replace probabilities with None if the token count is 0\n",
    "    for key, value in probabilities.items():\n",
    "        if value == 0:\n",
    "            probabilities[key] = None\n",
    "    for key, value in probabilities_stemmed.items():\n",
    "        if value == 0:\n",
    "            probabilities_stemmed[key] = None\n",
    "    \n",
    "    return {\n",
    "        \"without_stemming\": probabilities,\n",
    "        \"with_stemming\": probabilities_stemmed\n",
    "    }\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "e806a74b-dfa0-4f41-a8b6-9116e3fed57b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef process_file(args):\\n    topic, file_path = args\\n    with open(file_path, \\'r\\', encoding=\\'utf-8\\') as f:\\n        body_text = f.read()\\n        # Clean the text\\n        body_text = clean_txt(body_text)\\n        tokens = body_text.split()\\n        return topic, tokens\\n\\ndef word_prob(directory, column_name, token):\\n    # Normalize the token\\n    token_cleaned = clean_txt(token)\\n    token_stemmed = stem_fun(token_cleaned, \"stem\")\\n    \\n    # Initialize counters\\n    all_tokens = Counter()\\n    all_tokens_stemmed = Counter()\\n    topic_tokens = {\\n        \"Fishing\": Counter(),\\n        \"Hiking\": Counter(),\\n        \"MachineLearning\": Counter(),\\n        \"Mathematics\": Counter()\\n    }\\n    topic_tokens_stemmed = {\\n        \"Fishing\": Counter(),\\n        \"Hiking\": Counter(),\\n        \"MachineLearning\": Counter(),\\n        \"Mathematics\": Counter()\\n    }\\n    \\n    # Collect files for all topics\\n    files_to_process = []\\n    for topic in topic_tokens.keys():\\n        topic_path = os.path.join(directory, topic)\\n        if not os.path.exists(topic_path):\\n            raise ValueError(f\"Directory \\'{topic_path}\\' does not exist.\")\\n        \\n        for root, _, files in os.walk(topic_path):\\n            for file in files:\\n                if file.endswith(\".txt\"):\\n                    file_path = os.path.join(root, file)\\n                    files_to_process.append((topic, file_path))\\n    \\n    # Use multiprocessing to process files in parallel\\n    with Pool(processes=multiprocessing.cpu_count()) as pool:\\n        results = pool.map(process_file, files_to_process)\\n    \\n    # Update counters with results from parallel processing\\n    for topic, tokens in results:\\n        all_tokens.update(tokens)\\n        topic_tokens[topic].update(tokens)\\n        # Apply stemming to tokens\\n        stemmed_tokens = [stem_fun(token, \"stem\") for token in tokens]\\n        all_tokens_stemmed.update(stemmed_tokens)\\n        topic_tokens_stemmed[topic].update(stemmed_tokens)\\n    \\n    # Calculate probabilities\\n    def calc_probability(count, total):\\n        return count / total if total > 0 else None\\n\\n    # Probabilities without stemming\\n    all_count = sum(all_tokens.values())\\n    token_count_all = all_tokens[token_cleaned]\\n    probability_all = calc_probability(token_count_all, all_count)\\n    \\n    probabilities = {\\n        \"all\": probability_all,\\n        \"Fishing\": None,\\n        \"Hiking\": None,\\n        \"MachineLearning\": None,\\n        \"Mathematics\": None\\n    }\\n    \\n    for topic, counter in topic_tokens.items():\\n        total_tokens_topic = sum(counter.values())\\n        token_count_topic = counter[token_cleaned]\\n        probabilities[topic] = calc_probability(token_count_topic, total_tokens_topic)\\n    \\n    # Probabilities with stemming\\n    all_count_stemmed = sum(all_tokens_stemmed.values())\\n    token_count_all_stemmed = all_tokens_stemmed[token_stemmed]\\n    probability_all_stemmed = calc_probability(token_count_all_stemmed, all_count_stemmed)\\n    \\n    probabilities_stemmed = {\\n        \"all\": probability_all_stemmed,\\n        \"Fishing\": None,\\n        \"Hiking\": None,\\n        \"MachineLearning\": None,\\n        \"Mathematics\": None\\n    }\\n    \\n    for topic, counter in topic_tokens_stemmed.items():\\n        total_tokens_topic_stemmed = sum(counter.values())\\n        token_count_topic_stemmed = counter[token_stemmed]\\n        probabilities_stemmed[topic] = calc_probability(token_count_topic_stemmed, total_tokens_topic_stemmed)\\n    \\n    # Replace probabilities with None if the token count is 0\\n    for key, value in probabilities.items():\\n        if value == 0:\\n            probabilities[key] = None\\n    for key, value in probabilities_stemmed.items():\\n        if value == 0:\\n            probabilities_stemmed[key] = None\\n    \\n    # Create a DataFrame for better display\\n    data = []\\n    data.append([\"All\", probabilities[\"all\"], probabilities_stemmed[\"all\"]])\\n    for topic in topic_tokens.keys():\\n        data.append([topic, probabilities[topic], probabilities_stemmed[topic]])\\n    \\n    df = pd.DataFrame(data, columns=[\"Topic\", \"Probability Without Stemming\", \"Probability With Stemming\"])\\n    return df\\n'"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Version 5: Clean Up Results in Table\n",
    "\"\"\"\n",
    "def process_file(args):\n",
    "    topic, file_path = args\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        body_text = f.read()\n",
    "        # Clean the text\n",
    "        body_text = clean_txt(body_text)\n",
    "        tokens = body_text.split()\n",
    "        return topic, tokens\n",
    "\n",
    "def word_prob(directory, column_name, token):\n",
    "    # Normalize the token\n",
    "    token_cleaned = clean_txt(token)\n",
    "    token_stemmed = stem_fun(token_cleaned, \"stem\")\n",
    "    \n",
    "    # Initialize counters\n",
    "    all_tokens = Counter()\n",
    "    all_tokens_stemmed = Counter()\n",
    "    topic_tokens = {\n",
    "        \"Fishing\": Counter(),\n",
    "        \"Hiking\": Counter(),\n",
    "        \"MachineLearning\": Counter(),\n",
    "        \"Mathematics\": Counter()\n",
    "    }\n",
    "    topic_tokens_stemmed = {\n",
    "        \"Fishing\": Counter(),\n",
    "        \"Hiking\": Counter(),\n",
    "        \"MachineLearning\": Counter(),\n",
    "        \"Mathematics\": Counter()\n",
    "    }\n",
    "    \n",
    "    # Collect files for all topics\n",
    "    files_to_process = []\n",
    "    for topic in topic_tokens.keys():\n",
    "        topic_path = os.path.join(directory, topic)\n",
    "        if not os.path.exists(topic_path):\n",
    "            raise ValueError(f\"Directory '{topic_path}' does not exist.\")\n",
    "        \n",
    "        for root, _, files in os.walk(topic_path):\n",
    "            for file in files:\n",
    "                if file.endswith(\".txt\"):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    files_to_process.append((topic, file_path))\n",
    "    \n",
    "    # Use multiprocessing to process files in parallel\n",
    "    with Pool(processes=multiprocessing.cpu_count()) as pool:\n",
    "        results = pool.map(process_file, files_to_process)\n",
    "    \n",
    "    # Update counters with results from parallel processing\n",
    "    for topic, tokens in results:\n",
    "        all_tokens.update(tokens)\n",
    "        topic_tokens[topic].update(tokens)\n",
    "        # Apply stemming to tokens\n",
    "        stemmed_tokens = [stem_fun(token, \"stem\") for token in tokens]\n",
    "        all_tokens_stemmed.update(stemmed_tokens)\n",
    "        topic_tokens_stemmed[topic].update(stemmed_tokens)\n",
    "    \n",
    "    # Calculate probabilities\n",
    "    def calc_probability(count, total):\n",
    "        return count / total if total > 0 else None\n",
    "\n",
    "    # Probabilities without stemming\n",
    "    all_count = sum(all_tokens.values())\n",
    "    token_count_all = all_tokens[token_cleaned]\n",
    "    probability_all = calc_probability(token_count_all, all_count)\n",
    "    \n",
    "    probabilities = {\n",
    "        \"all\": probability_all,\n",
    "        \"Fishing\": None,\n",
    "        \"Hiking\": None,\n",
    "        \"MachineLearning\": None,\n",
    "        \"Mathematics\": None\n",
    "    }\n",
    "    \n",
    "    for topic, counter in topic_tokens.items():\n",
    "        total_tokens_topic = sum(counter.values())\n",
    "        token_count_topic = counter[token_cleaned]\n",
    "        probabilities[topic] = calc_probability(token_count_topic, total_tokens_topic)\n",
    "    \n",
    "    # Probabilities with stemming\n",
    "    all_count_stemmed = sum(all_tokens_stemmed.values())\n",
    "    token_count_all_stemmed = all_tokens_stemmed[token_stemmed]\n",
    "    probability_all_stemmed = calc_probability(token_count_all_stemmed, all_count_stemmed)\n",
    "    \n",
    "    probabilities_stemmed = {\n",
    "        \"all\": probability_all_stemmed,\n",
    "        \"Fishing\": None,\n",
    "        \"Hiking\": None,\n",
    "        \"MachineLearning\": None,\n",
    "        \"Mathematics\": None\n",
    "    }\n",
    "    \n",
    "    for topic, counter in topic_tokens_stemmed.items():\n",
    "        total_tokens_topic_stemmed = sum(counter.values())\n",
    "        token_count_topic_stemmed = counter[token_stemmed]\n",
    "        probabilities_stemmed[topic] = calc_probability(token_count_topic_stemmed, total_tokens_topic_stemmed)\n",
    "    \n",
    "    # Replace probabilities with None if the token count is 0\n",
    "    for key, value in probabilities.items():\n",
    "        if value == 0:\n",
    "            probabilities[key] = None\n",
    "    for key, value in probabilities_stemmed.items():\n",
    "        if value == 0:\n",
    "            probabilities_stemmed[key] = None\n",
    "    \n",
    "    # Create a DataFrame for better display\n",
    "    data = []\n",
    "    data.append([\"All\", probabilities[\"all\"], probabilities_stemmed[\"all\"]])\n",
    "    for topic in topic_tokens.keys():\n",
    "        data.append([topic, probabilities[topic], probabilities_stemmed[topic]])\n",
    "    \n",
    "    df = pd.DataFrame(data, columns=[\"Topic\", \"Probability Without Stemming\", \"Probability With Stemming\"])\n",
    "    return df\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda-2024.02-py310",
   "language": "python",
   "name": "conda-env-anaconda-2024.02-py310-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
