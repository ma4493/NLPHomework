{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc466a09-5aaa-4498-b03b-03107fe9e7c0",
   "metadata": {},
   "source": [
    "**Mateo Alexander**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc32b38b-d69b-4af5-973c-baf48fc58c91",
   "metadata": {},
   "source": [
    "**Homework 4**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f71bdb-fe77-4bff-ab55-7f42c0002e40",
   "metadata": {},
   "source": [
    "**Natural Language Processing: QMSS 5067**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d249146-d02f-4033-8c33-51701777a15d",
   "metadata": {},
   "source": [
    "**Professor Patrick Houlihan**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f156bf9c-a545-4a00-bfba-d3390933985d",
   "metadata": {},
   "source": [
    "**Homework Due 12/02/2024**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6c6e0a1-1656-4c04-b3ce-3f0f292fd8c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-30T21:19:54.973235Z",
     "iopub.status.busy": "2024-12-30T21:19:54.972910Z",
     "iopub.status.idle": "2024-12-30T21:19:54.978504Z",
     "shell.execute_reply": "2024-12-30T21:19:54.977910Z",
     "shell.execute_reply.started": "2024-12-30T21:19:54.973213Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nDocument Classification Case Study: TechCorp Document Management System\\nBackground\\nTechCorp is a growing technology company with thousands of documents scattered across their systems. These documents include legal contracts, marketing materials, and engineering specifications. Currently, employees manually categorize these documents, which is time-consuming and prone to errors. The company wants to implement an automated solution using machine learning to classify documents into three categories: Legal, Marketing, and Engineering.\\nAvailable Data\\nTechCorp has provided you with:\\nA training dataset, hw4.pk, of pre-labeled documents, all in .txt form:\\nlegal documents (contracts, agreements, compliance reports)\\nmarketing documents (brochures, campaign materials, social media content)\\nengineering documents (technical specifications, code documentation, design docs)\\nEach document is provided in text format\\nLabels for each document in the training set\\nAs a data science student, your task is to:\\n- Provide code that that solves for an automated end-to-end solution for this problem.\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Document Classification Case Study: TechCorp Document Management System\n",
    "Background\n",
    "TechCorp is a growing technology company with thousands of documents scattered across their systems. These documents include legal contracts, marketing materials, and engineering specifications. Currently, employees manually categorize these documents, which is time-consuming and prone to errors. The company wants to implement an automated solution using machine learning to classify documents into three categories: Legal, Marketing, and Engineering.\n",
    "Available Data\n",
    "TechCorp has provided you with:\n",
    "A training dataset, hw4.pk, of pre-labeled documents, all in .txt form:\n",
    "legal documents (contracts, agreements, compliance reports)\n",
    "marketing documents (brochures, campaign materials, social media content)\n",
    "engineering documents (technical specifications, code documentation, design docs)\n",
    "Each document is provided in text format\n",
    "Labels for each document in the training set\n",
    "As a data science student, your task is to:\n",
    "- Provide code that that solves for an automated end-to-end solution for this problem.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "c624bc21-cf80-4f72-9fa2-0c2dc748c556",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-03T02:26:30.528269Z",
     "iopub.status.busy": "2024-12-03T02:26:30.527935Z",
     "iopub.status.idle": "2024-12-03T02:26:30.534669Z",
     "shell.execute_reply": "2024-12-03T02:26:30.533827Z",
     "shell.execute_reply.started": "2024-12-03T02:26:30.528236Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier, StackingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.utils import resample\n",
    "import numpy as np\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1b0063-ba5d-4880-af90-f789430c9faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dac4de44-1d50-4075-8ccd-6013d2c5fae2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T03:10:56.683695Z",
     "iopub.status.busy": "2024-12-02T03:10:56.683481Z",
     "iopub.status.idle": "2024-12-02T03:10:56.697891Z",
     "shell.execute_reply": "2024-12-02T03:10:56.697296Z",
     "shell.execute_reply.started": "2024-12-02T03:10:56.683677Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/37b4d573-02a2-4c75-b69b-8f21f8c5d212/Natural Language Processing/hw4.pk\n"
     ]
    }
   ],
   "source": [
    "file_path_data = os.path.abspath('hw4.pk')\n",
    "print(file_path_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c44f17b5-26ad-4ce9-b9e3-9603a3a3126a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T21:34:49.099554Z",
     "iopub.status.busy": "2024-12-02T21:34:49.099287Z",
     "iopub.status.idle": "2024-12-02T21:34:49.129898Z",
     "shell.execute_reply": "2024-12-02T21:34:49.129093Z",
     "shell.execute_reply.started": "2024-12-02T21:34:49.099533Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: Index(['body', 'label'], dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We use essential cookies to make Venngage wor...</td>\n",
       "      <td>legal_contract_examples</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A legal contract is a written document that is...</td>\n",
       "      <td>legal_contract_examples</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>November 27 2023 14 min Author Olga Asheychik...</td>\n",
       "      <td>legal_contract_examples</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Accelerate contracts with AI native workflows ...</td>\n",
       "      <td>legal_contract_examples</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Create smarter agreements commit to them more ...</td>\n",
       "      <td>legal_contract_examples</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>In software development almost everyone you wo...</td>\n",
       "      <td>engineering_specification_examples</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>You have been blocked If you believe this in e...</td>\n",
       "      <td>engineering_specification_examples</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>Denis Sheremetov CTO at Onix Mila Slesar Write...</td>\n",
       "      <td>engineering_specification_examples</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>Engineering Team June 6 2024 18min read The b...</td>\n",
       "      <td>engineering_specification_examples</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>Official websites use mil Secure mil websites...</td>\n",
       "      <td>engineering_specification_examples</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  body  \\\n",
       "0     We use essential cookies to make Venngage wor...   \n",
       "1    A legal contract is a written document that is...   \n",
       "2     November 27 2023 14 min Author Olga Asheychik...   \n",
       "3    Accelerate contracts with AI native workflows ...   \n",
       "4    Create smarter agreements commit to them more ...   \n",
       "..                                                 ...   \n",
       "195  In software development almost everyone you wo...   \n",
       "196  You have been blocked If you believe this in e...   \n",
       "197  Denis Sheremetov CTO at Onix Mila Slesar Write...   \n",
       "198   Engineering Team June 6 2024 18min read The b...   \n",
       "199   Official websites use mil Secure mil websites...   \n",
       "\n",
       "                                  label  \n",
       "0               legal_contract_examples  \n",
       "1               legal_contract_examples  \n",
       "2               legal_contract_examples  \n",
       "3               legal_contract_examples  \n",
       "4               legal_contract_examples  \n",
       "..                                  ...  \n",
       "195  engineering_specification_examples  \n",
       "196  engineering_specification_examples  \n",
       "197  engineering_specification_examples  \n",
       "198  engineering_specification_examples  \n",
       "199  engineering_specification_examples  \n",
       "\n",
       "[200 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the data from the .pk file using the absolute path\n",
    "file_path = '/home/37b4d573-02a2-4c75-b69b-8f21f8c5d212/Natural Language Processing/hw4.pk'\n",
    "with open(file_path, 'rb') as file:\n",
    "    data = pickle.load(file)\n",
    "\n",
    "# Displaying the columns and the first few rows to inspect the DataFrame\n",
    "print(\"Columns:\", data.columns)\n",
    "display(data.head(200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "66d2d9ff-a48a-4d8e-9c32-27868d105e6e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-03T01:58:40.629646Z",
     "iopub.status.busy": "2024-12-03T01:58:40.629024Z",
     "iopub.status.idle": "2024-12-03T02:12:26.542571Z",
     "shell.execute_reply": "2024-12-03T02:12:26.541729Z",
     "shell.execute_reply.started": "2024-12-03T01:58:40.629611Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating RandomForestClassifier with Hyperparameter Tuning...\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Best Parameters: {'n_estimators': 100, 'max_features': 'log2', 'max_depth': None}\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Engineering       0.86      1.00      0.92         6\n",
      "       Legal       1.00      0.82      0.90        11\n",
      "   Marketing       0.91      1.00      0.95        10\n",
      "\n",
      "    accuracy                           0.93        27\n",
      "   macro avg       0.92      0.94      0.93        27\n",
      "weighted avg       0.93      0.93      0.92        27\n",
      "\n",
      "Accuracy Score: 0.9259259259259259\n",
      "\n",
      "Evaluating GradientBoostingClassifier with Hyperparameter Tuning...\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Best Parameters: {'n_estimators': 200, 'max_depth': 3, 'learning_rate': 0.5}\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Engineering       0.75      1.00      0.86         6\n",
      "       Legal       1.00      0.91      0.95        11\n",
      "   Marketing       1.00      0.90      0.95        10\n",
      "\n",
      "    accuracy                           0.93        27\n",
      "   macro avg       0.92      0.94      0.92        27\n",
      "weighted avg       0.94      0.93      0.93        27\n",
      "\n",
      "Accuracy Score: 0.9259259259259259\n",
      "\n",
      "Evaluating XGBClassifier with Hyperparameter Tuning...\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Best Parameters: {'n_estimators': 100, 'max_depth': 5, 'learning_rate': 0.1}\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Engineering       0.75      1.00      0.86         6\n",
      "       Legal       1.00      0.82      0.90        11\n",
      "   Marketing       1.00      1.00      1.00        10\n",
      "\n",
      "    accuracy                           0.93        27\n",
      "   macro avg       0.92      0.94      0.92        27\n",
      "weighted avg       0.94      0.93      0.93        27\n",
      "\n",
      "Accuracy Score: 0.9259259259259259\n",
      "\n",
      "Evaluating Voting Classifier with RandomForest, GradientBoosting, and XGBoost...\n",
      "Classification Report for Voting Classifier:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Engineering       0.75      1.00      0.86         6\n",
      "       Legal       1.00      0.82      0.90        11\n",
      "   Marketing       1.00      1.00      1.00        10\n",
      "\n",
      "    accuracy                           0.93        27\n",
      "   macro avg       0.92      0.94      0.92        27\n",
      "weighted avg       0.94      0.93      0.93        27\n",
      "\n",
      "Accuracy Score: 0.9259259259259259\n",
      "\n",
      "Evaluating Stacking Classifier with RandomForest, GradientBoosting, and XGBoost...\n",
      "Classification Report for Stacking Classifier:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Engineering       0.86      1.00      0.92         6\n",
      "       Legal       1.00      0.91      0.95        11\n",
      "   Marketing       1.00      1.00      1.00        10\n",
      "\n",
      "    accuracy                           0.96        27\n",
      "   macro avg       0.95      0.97      0.96        27\n",
      "weighted avg       0.97      0.96      0.96        27\n",
      "\n",
      "Accuracy Score: 0.9629629629629629\n",
      "\n",
      "Evaluating Stacking Classifier with 5-Fold Cross-Validation...\n",
      "Cross-Validation Scores: [0.7826087  0.95652174 0.91304348 0.82608696 0.95454545]\n",
      "Mean Cross-Validation Score: 0.8865612648221344\n",
      "\n",
      "Results DataFrame:\n",
      "                                             body_text actual_category  \\\n",
      "88   website us cooky provide better user experienc...       Marketing   \n",
      "73   find making costly mistake fix get grade get r...       Marketing   \n",
      "47   know essential element contract legally bindin...           Legal   \n",
      "39   occur following reason disabling removing brow...           Legal   \n",
      "145  published kimberley derudder welcome real esta...       Marketing   \n",
      "\n",
      "    category_expected  correct_classification  \n",
      "88          Marketing                    True  \n",
      "73          Marketing                    True  \n",
      "47              Legal                    True  \n",
      "39        Engineering                   False  \n",
      "145         Marketing                    True  \n"
     ]
    }
   ],
   "source": [
    "# FINAL CODE: Classification Tool\n",
    "\n",
    "# Load the dataset\n",
    "file_path = '/home/37b4d573-02a2-4c75-b69b-8f21f8c5d212/Natural Language Processing/hw4.pk'\n",
    "with open(file_path, 'rb') as file:\n",
    "    data = pickle.load(file)\n",
    "\n",
    "# Adding a new column 'category' based on the value of the 'label' column\n",
    "data['category'] = data['label'].map({\n",
    "    'legal_contract_examples': 'Legal',\n",
    "    'marketing_material_examples': 'Marketing',\n",
    "    'engineering_specification_examples': 'Engineering'\n",
    "})\n",
    "\n",
    "# Create a new DataFrame with only 'category' and 'body' columns for data analysis\n",
    "analysis_df = data[['category', 'body']]\n",
    "\n",
    "# Text Preprocessing Function\n",
    "def preprocess_text(text):\n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'[^a-zA-Z]', ' ', text)\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Tokenize and remove stopwords\n",
    "    words = text.split()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    # Lemmatize words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Apply preprocessing to the 'body' column\n",
    "analysis_df.loc[:, 'body'] = analysis_df['body'].apply(preprocess_text)\n",
    "\n",
    "# Balancing the dataset by taking 20% of each classification category\n",
    "legal_sample = resample(analysis_df[analysis_df['category'] == 'Legal'],\n",
    "                        replace=False, n_samples=int(len(analysis_df) * 0.2), random_state=42)\n",
    "marketing_sample = resample(analysis_df[analysis_df['category'] == 'Marketing'],\n",
    "                            replace=False, n_samples=int(len(analysis_df) * 0.2), random_state=42)\n",
    "engineering_sample = resample(analysis_df[analysis_df['category'] == 'Engineering'],\n",
    "                              replace=False, n_samples=int(len(analysis_df) * 0.2), random_state=42)\n",
    "\n",
    "# Concatenate the samples to create a balanced dataset\n",
    "balanced_df = pd.concat([legal_sample, marketing_sample, engineering_sample])\n",
    "\n",
    "# Splitting the balanced dataset into training and test sets\n",
    "X = balanced_df['body']\n",
    "y = balanced_df['category']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert text data to numerical data using TF-IDF Vectorizer with n-grams\n",
    "tfidf = TfidfVectorizer(stop_words='english', max_df=0.85, ngram_range=(1, 3), max_features=5000)\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "# Apply SMOTE to handle class imbalance\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train_tfidf, y_train)\n",
    "\n",
    "# Encode labels for XGBoost\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_smote_encoded = label_encoder.fit_transform(y_train_smote)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "# Feature Engineering: Adding Named Entity Counts and Document Length\n",
    "X_train_len = X_train.apply(lambda x: len(x.split())).values.reshape(-1, 1)\n",
    "X_test_len = X_test.apply(lambda x: len(x.split())).values.reshape(-1, 1)\n",
    "\n",
    "# Combine TF-IDF features with document length for training and testing sets\n",
    "X_train_with_features = np.hstack((X_train_tfidf.toarray(), X_train_len))\n",
    "X_test_with_features = np.hstack((X_test_tfidf.toarray(), X_test_len))\n",
    "\n",
    "# Apply PCA for dimensionality reduction (except for MultinomialNB)\n",
    "pca = PCA(n_components=0.95, random_state=42)\n",
    "X_train_pca = pca.fit_transform(X_train_smote.toarray())\n",
    "X_test_pca = pca.transform(X_test_tfidf.toarray())\n",
    "\n",
    "# Defining classifiers to evaluate with hyperparameter tuning\n",
    "param_grid = {\n",
    "    'RandomForestClassifier': {\n",
    "        'n_estimators': [100, 200, 500],\n",
    "        'max_depth': [10, 20, None],\n",
    "        'max_features': ['sqrt', 'log2', None]\n",
    "    },\n",
    "    'GradientBoostingClassifier': {\n",
    "        'n_estimators': [100, 200],\n",
    "        'learning_rate': [0.01, 0.1, 0.5],\n",
    "        'max_depth': [3, 5, 7]\n",
    "    },\n",
    "    'XGBClassifier': {\n",
    "        'n_estimators': [100, 200],\n",
    "        'learning_rate': [0.01, 0.1, 0.5],\n",
    "        'max_depth': [3, 5, 7]\n",
    "    }\n",
    "}\n",
    "\n",
    "classifiers = {\n",
    "    'RandomForestClassifier': RandomForestClassifier(),\n",
    "    'GradientBoostingClassifier': GradientBoostingClassifier(),\n",
    "    'XGBClassifier': XGBClassifier(eval_metric='mlogloss')\n",
    "}\n",
    "\n",
    "# Evaluating performance for RandomForest, GradientBoosting, and XGBoost using RandomizedSearchCV\n",
    "for name, clf in classifiers.items():\n",
    "    print(f\"\\nEvaluating {name} with Hyperparameter Tuning...\")\n",
    "    randomized_search = RandomizedSearchCV(clf, param_grid[name], n_iter=10, cv=3, n_jobs=-1, verbose=2, random_state=42)\n",
    "    if name == 'XGBClassifier':\n",
    "        randomized_search.fit(X_train_pca, y_train_smote_encoded)\n",
    "    else:\n",
    "        randomized_search.fit(X_train_pca, y_train_smote)\n",
    "    best_clf = randomized_search.best_estimator_\n",
    "    y_pred = best_clf.predict(X_test_pca)\n",
    "    y_pred = label_encoder.inverse_transform(y_pred) if name == 'XGBClassifier' else y_pred\n",
    "    print(\"Best Parameters:\", randomized_search.best_params_)\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"Accuracy Score:\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "# Ensemble Voting Classifier using the top-performing models\n",
    "print(\"\\nEvaluating Voting Classifier with RandomForest, GradientBoosting, and XGBoost...\")\n",
    "voting_clf = VotingClassifier(estimators=[\n",
    "    ('rf', RandomForestClassifier(n_estimators=200, max_depth=None, max_features='log2', random_state=42)),\n",
    "    ('gb', GradientBoostingClassifier(n_estimators=200, learning_rate=0.5, max_depth=3, random_state=42)),\n",
    "    ('xgb', XGBClassifier(n_estimators=200, learning_rate=0.5, max_depth=3, eval_metric='mlogloss', random_state=42))\n",
    "], voting='soft', weights=[2, 1, 1])\n",
    "\n",
    "voting_clf.fit(X_train_pca, y_train_smote_encoded)\n",
    "y_pred_voting = voting_clf.predict(X_test_pca)\n",
    "y_pred_voting = label_encoder.inverse_transform(y_pred_voting)\n",
    "print(\"Classification Report for Voting Classifier:\")\n",
    "print(classification_report(y_test, y_pred_voting))\n",
    "print(\"Accuracy Score:\", accuracy_score(y_test, y_pred_voting))\n",
    "\n",
    "# Stacking Classifier to combine the models\n",
    "print(\"\\nEvaluating Stacking Classifier with RandomForest, GradientBoosting, and XGBoost...\")\n",
    "stacking_clf = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('rf', RandomForestClassifier(n_estimators=200, max_depth=None, max_features='log2', random_state=42)),\n",
    "        ('gb', GradientBoostingClassifier(n_estimators=200, learning_rate=0.5, max_depth=3, random_state=42)),\n",
    "        ('xgb', XGBClassifier(n_estimators=200, learning_rate=0.5, max_depth=3, eval_metric='mlogloss', random_state=42))\n",
    "    ],\n",
    "    final_estimator=LogisticRegression(max_iter=1000)\n",
    ")\n",
    "\n",
    "stacking_clf.fit(X_train_pca, y_train_smote_encoded)\n",
    "y_pred_stacking = stacking_clf.predict(X_test_pca)\n",
    "y_pred_stacking = label_encoder.inverse_transform(y_pred_stacking)\n",
    "print(\"Classification Report for Stacking Classifier:\")\n",
    "print(classification_report(y_test, y_pred_stacking))\n",
    "print(\"Accuracy Score:\", accuracy_score(y_test, y_pred_stacking))\n",
    "\n",
    "# 5-Fold Cross-Validation for the best model (Stacking Classifier)\n",
    "print(\"\\nEvaluating Stacking Classifier with 5-Fold Cross-Validation...\")\n",
    "cross_val_scores = cross_val_score(stacking_clf, X_train_pca, y_train_smote_encoded, cv=5, n_jobs=-1)\n",
    "print(\"Cross-Validation Scores:\", cross_val_scores)\n",
    "print(\"Mean Cross-Validation Score:\", np.mean(cross_val_scores))\n",
    "\n",
    "# Creating a new DataFrame with predicted categories and evaluation of correctness\n",
    "results_df = pd.DataFrame({\n",
    "    'body_text': X_test,\n",
    "    'actual_category': y_test,\n",
    "    'category_expected': y_pred_stacking\n",
    "})\n",
    "results_df['correct_classification'] = results_df['actual_category'] == results_df['category_expected']\n",
    "\n",
    "# Displaying the results DataFrame\n",
    "print(\"\\nResults DataFrame:\")\n",
    "print(results_df.head())\n",
    "\n",
    "# Export the results DataFrame to a CSV file\n",
    "results_df.to_csv('/home/37b4d573-02a2-4c75-b69b-8f21f8c5d212/Natural Language Processing/classification_results.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda-ai-2024.04-py310",
   "language": "python",
   "name": "conda-env-anaconda-ai-2024.04-py310-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
